{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoVkS7Iywtp9",
        "outputId": "dd290db7-2610-41c7-d965-019dd3b40de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (1.82.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.2)\n",
            "Requirement already satisfied: certifi in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2021.10.8)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Requirement already satisfied: anthropic in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (0.52.0)\n",
            "Requirement already satisfied: sniffio in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anthropic) (2.7.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->anthropic) (3.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.5)\n",
            "Requirement already satisfied: certifi in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.25.0->anthropic) (2021.10.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.18.2)\n",
            "Collecting huggingface_hub[cli]\n",
            "  Downloading huggingface_hub-0.32.0-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 2.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub[cli]) (6.0)\n",
            "Requirement already satisfied: filelock in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub[cli]) (3.17.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 31.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub[cli]) (4.11.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub[cli]) (4.62.3)\n",
            "Requirement already satisfied: requests in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub[cli]) (2.32.1)\n",
            "Collecting fsspec>=2023.5.0\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 103.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from huggingface_hub[cli]) (24.0)\n",
            "Collecting InquirerPy==0.3.4\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 19.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.20)\n",
            "Collecting pfzy<0.4.0,>=0.3.1\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: wcwidth in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/anika/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (3.2)\n",
            "Installing collected packages: pfzy, hf-xet, fsspec, InquirerPy, huggingface-hub\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2021.8.1\n",
            "    Uninstalling fsspec-2021.8.1:\n",
            "      Successfully uninstalled fsspec-2021.8.1\n",
            "Successfully installed InquirerPy-0.3.4 fsspec-2025.5.1 hf-xet-1.1.2 huggingface-hub-0.32.0 pfzy-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install anthropic\n",
        "!pip install -q -U google-genai\n",
        "!pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wREGUu0Wpb4s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from openai import OpenAI\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import anthropic\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "import collections\n",
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye-5xiXC3IiY"
      },
      "source": [
        "## Load in REVEAL dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJlsUkUYptJ7",
        "outputId": "ff8bd1c8-ec91-4584-99b9-8a1469e779a8"
      },
      "outputs": [],
      "source": [
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "# TODO: GET A TOKEN FOR HUGGINGFACE\n",
        "\n",
        "# Use this if on Google Colab\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c4e1d8ae50b49439cdd44a0c816c1f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Use this if on VS Code / Jupyter\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yZQZsiDvpoo6"
      },
      "outputs": [],
      "source": [
        "# Load in REVEAL dataset\n",
        "splits = {'eval': 'eval/reveal_eval.csv', 'open': 'open/reveal_open.csv'}\n",
        "df = pd.read_csv('hf://datasets/google/reveal/' + splits['eval'])\n",
        "df = df.drop_duplicates(subset=['answer_id', 'step_idx']).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfjFKRZw3O_p",
        "outputId": "d215d219-c9f9-4d44-f8a2-f27ad3cd59cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets in eval set: ['fermi' 'musique' 'sports' 'strategy_qa']\n",
            "Models tested in eval set: ['GPT-3' 'Flan-PaLM-540B' 'Flan-UL2-20B']\n",
            "Number of questions in eval set: 704\n",
            "Number of answers in eval set: 1002\n"
          ]
        }
      ],
      "source": [
        "print(f\"Datasets in eval set: {df['dataset'].unique()}\")\n",
        "print(f\"Models tested in eval set: {df['answer_model'].unique()}\")\n",
        "print(f\"Number of questions in eval set: {len(df['question_id'].unique())}\")\n",
        "print(f\"Number of answers in eval set: {len(df['answer_id'].unique())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ZlHflU5nQB",
        "outputId": "2d37f53c-6cd7-4001-c988-66fadd970eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type_label: ['Attribution step.' 'Logical step.' 'Both.']\n",
            "logic_relevance_label: ['Relevant' 'Not relevant']\n",
            "attribution_relevance_label: ['Yes' nan 'No']\n",
            "attribution_label: ['Partially' 'Unsupported' nan 'Fully' 'Contradictory']\n",
            "correctness_label: [nan 'Incorrect' 'Correct']\n",
            "answer_is_fully_attributable: [False  True]\n",
            "answer_is_logically_correct: [False  True]\n"
          ]
        }
      ],
      "source": [
        "# What are the values in the relevant columns?\n",
        "group_keys = ['question_id', 'answer_id', 'answer_model']\n",
        "step_keys = ['question', 'step', 'step_idx']\n",
        "metric_cols = [\n",
        "    'type_label', 'logic_relevance_label', 'attribution_relevance_label',\n",
        "    'attribution_label', 'correctness_label', 'evidence',\n",
        "    'answer_is_fully_attributable', 'answer_is_logically_correct'\n",
        "]\n",
        "justification_cols = ['logic_justifications', 'attribution_justifications']\n",
        "\n",
        "for col in metric_cols:\n",
        "  if col == 'evidence':\n",
        "    continue\n",
        "  print(f\"{col}: {df[col].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d_LqupzC6ugz"
      },
      "outputs": [],
      "source": [
        "# Create nested dictionary. Each key is a tuple (question_id, answer_model, answer_id). Values are a list of steps and their labels.\n",
        "grouped_data = defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    group_id = tuple(row[k] for k in group_keys)\n",
        "    step_info = {k: row[k] for k in step_keys + metric_cols + justification_cols}\n",
        "    grouped_data[group_id].append(step_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTY8ru9UwFsB",
        "outputId": "d421578e-6284-4782-a05d-c5ac897bb85e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Is the following sentence plausible? \"David Luiz shot with the left foot.\"\n",
            "Answer Model: sports/366//Flan-PaLM-540B\n",
            "-----\n",
            "Step 1\n",
            "Step Text: David Luiz is a soccer player.\n",
            "-----\n",
            "Step 2\n",
            "Step Text: Soccer players can shoot with the left foot.\n",
            "-----\n",
            "Step 3\n",
            "Step Text: So the answer is yes.\n"
          ]
        }
      ],
      "source": [
        "# Example of random question and CoT steps with annotations\n",
        "random_key = random.choice(list(grouped_data.keys()))\n",
        "step_info = grouped_data[random_key]\n",
        "\n",
        "print(f\"Question: {step_info[0]['question']}\")\n",
        "print(f\"Answer Model: {random_key[1]}\")\n",
        "\n",
        "for i in range(len(step_info)):\n",
        "  print('-----')\n",
        "  print(f\"Step {step_info[i]['step_idx']}\")\n",
        "  print(f\"Step Text: {step_info[i]['step']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ-WOWZT8itq",
        "outputId": "ea2bca5f-7936-46cc-e4f5-4d33e826d3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Is the following sentence plausible? \"Tobias Harris fumbled the ball in the NFC divisional round.\"\n",
            "Answer Model: sports/137//Flan-UL2-20B\n",
            "-----\n",
            "Step 1\n",
            "Step Text: Tobias Harris is a basketball player.\n",
            "Step Type: Attribution step.\n",
            "\n",
            ">> TASK 1: LOGIC ANNOTATION\n",
            "- Logical Relevance to Question: Relevant\n",
            "- Logical Correctness: nan\n",
            "- Logic Justification: ['The question is about Tobias Harris, so it could be relevant that is a basketball player.\\n', 'Step one states who Harris is ', 'brings new info on what sport Harris plays.', 'Relevant because it provides the sport that Harris plays, which is necessary to know to answer the question.', 'Step 1 adds that Tobias Harris is a basketball player.']\n",
            "\n",
            ">> TASK 2: ATTRIBUTION ANNOTATION\n",
            "- Evidence: Tobias Harris: Tobias Harris (born July 15, 1992) is an American professional basketball player for the Philadelphia 76ers of the National Basketball Association (NBA). He played one season of college basketball for the Tennessee Volunteers before declaring for the 2011 NBA draft where he was drafted 19th overall by the Charlotte Bobcats and then traded to the Milwaukee Bucks. Harris has also played for the Orlando Magic, Detroit Pistons and Los Angeles Clippers.\n",
            "- Attribution Needed: Yes\n",
            "- Attribution Label: Fully\n",
            "- Attribution Justification: ['The evidence states \"Tobias Harris is an American professional basketball player for the Philadelphia 76ers of the National Basketball Association.\"  This supports the claim that he is a basketball player.', 'The evidence states that Harris is an American professional basketball player', 'The evidence mentions Tobias Harris is an American professional basketball player, so it could support the claim.  ', 'E1 fully supports the claim that Tobias Harris is a professional basketball player and provides details about his career history.', 'Step 0 is fully supported by the evidence stating that Harris is in the NBA.']\n",
            "\n",
            "-----\n",
            "Step 2\n",
            "Step Text: Fumbling the ball is part of basketball, not American football.\n",
            "Step Type: Attribution step.\n",
            "\n",
            ">> TASK 1: LOGIC ANNOTATION\n",
            "- Logical Relevance to Question: Relevant\n",
            "- Logical Correctness: nan\n",
            "- Logic Justification: ['The question mentions \"Tobias Harris fumbled the ball in the NFC divisional round.\" so it could be relevant that is part of basketball, not American football.', 'Step 2 states what sport uses fumbling the ball and which one does not ', 'Brings new info on what sport involves fumbling the ball.', 'Relevant because it provides the sport that fumbling the ball occurs in, which is also necessary to know to answer the question.', 'Step 2 adds that Fumbling the ball is part of basketball, not American football.']\n",
            "\n",
            ">> TASK 2: ATTRIBUTION ANNOTATION\n",
            "- Evidence: Fumble: Under American rules a fumble may be confused with a muff. A muff occurs where a player drops a ball that he does not have clear possession of, such as while attempting to catch a lateral pass or improperly fielding a kicking play such as a punt (you cannot \"fumble\" a loose ball). Ball security is the ability of a player to maintain control over the football during play and thus avoid a fumble. Thus, losing possession of the ball via a fumble includes not only dropping the ball before being downed; but, also having a ball taken away, or “stripped” from the runner's possession before being downed.\n",
            "- Attribution Needed: Yes\n",
            "- Attribution Label: Unsupported\n",
            "- Attribution Justification: ['The first one talks about fumbling and says football but never really states if fumbling is used in American football or not, never talks about basketball. The second one talks about fumbling and goes into an example in American Football, making the second part of the step( not American football) wrong as we can deduct that fumbling is in American football. The evidence states nothing about basketball ', 'The evidence mentions fumbling the ball in American football so it could contradict the claim.', 'E2 is contradictory because it says that fumbling does occur in American football whereas step 1 says that it is part of basektball.', 'The evidence states that fumbling the ball is part of American football. ', 'Evidence describes a fumble as being a part of American football.']\n",
            "\n",
            "-----\n",
            "Step 3\n",
            "Step Text: So the answer is no.\n",
            "Step Type: Logical step.\n",
            "\n",
            ">> TASK 1: LOGIC ANNOTATION\n",
            "- Logical Relevance to Question: Relevant\n",
            "- Logical Correctness: Incorrect\n",
            "- Logic Justification: ['It infers the answer is no. However, based on past steps, Tobias Harris is a basketball player, and \\nFumbling the ball is part of basketball. With the information so far the sentence could be plausible. ', 'According to step 2, the answer should be yes(before we talk about the NFC divisional round), though the steps never talk about the NFC divisional round ', 'Correct answer, but incorrect logic as it states that Harris is a basketball player and fumbling is part of basketball, but does not state whether or not the NFC divisional round is part of basketball. ', 'Incorrect logical inference because there is not enough information provided, such as if the NFC divisional round is also part of basketball.', 'Step 3 correctly infers that the sentence is not plausible based on step 2 stating that Fumbling the ball is part of basketball, not American football.']\n",
            "\n",
            ">> TASK 2: ATTRIBUTION ANNOTATION\n",
            "- Evidence: nan\n",
            "- Attribution Needed: nan\n",
            "- Attribution Label: nan\n",
            "- Attribution Justification: nan\n",
            "\n",
            "-----\n",
            "=== FINAL ANSWER ANNOTATIONS ===\n",
            "- Fully Attributable: False\n",
            "- Logically Correct: False\n"
          ]
        }
      ],
      "source": [
        "# Example of random question and CoT steps with annotations\n",
        "random_key = random.choice(list(grouped_data.keys()))\n",
        "step_info = grouped_data[random_key]\n",
        "\n",
        "print(f\"Question: {step_info[0]['question']}\")\n",
        "print(f\"Answer Model: {random_key[1]}\")\n",
        "\n",
        "for i in range(len(step_info)):\n",
        "  print('-----')\n",
        "  print(f\"Step {step_info[i]['step_idx']}\")\n",
        "  print(f\"Step Text: {step_info[i]['step']}\")\n",
        "  print(f\"Step Type: {step_info[i]['type_label']}\")\n",
        "  # Does step i bring new information or describe a logical step?\n",
        "  # (Attribution step. / Logical Step. / Both.)\n",
        "\n",
        "  print('\\n>> TASK 1: LOGIC ANNOTATION')\n",
        "  print(f\"- Logical Relevance to Question: {step_info[i]['logic_relevance_label']}\")\n",
        "  # Is this step relevant with respect to answering the question?\n",
        "  # Irrelevant steps do not invalidate the chain's correctness.\n",
        "  # (Relevant / Not Relevant)\n",
        "\n",
        "  print(f\"- Logical Correctness: {step_info[i]['correctness_label']}\")\n",
        "  # Considering only the logical inference done in step i, is it consistent with the previous steps?\n",
        "  # (Correct / Incorrect)\n",
        "  # The correctness of logical steps that follow incorrect logical steps is undefined.\n",
        "  # Does not apply to attribution steps.\n",
        "\n",
        "  print(f\"- Logic Justification: {step_info[i]['logic_justifications']}\")\n",
        "\n",
        "  print('\\n>> TASK 2: ATTRIBUTION ANNOTATION')\n",
        "  print(f\"- Evidence: {step_info[i].get('evidence', '[None]')}\")\n",
        "  # Retrieved Wikipedia paragraphs given as evidence if this is an attribution step.\n",
        "  # Does not apply to logic steps.\n",
        "\n",
        "  print(f\"- Attribution Needed: {step_info[i]['attribution_relevance_label']}\")\n",
        "  # Should claim i be attributed? Does the claim have information that needs to be verified?\n",
        "  # (Yes / No). Does not apply to logic steps.\n",
        "\n",
        "  print(f\"- Attribution Label: {step_info[i]['attribution_label']}\")\n",
        "  # To what extent can the information in claim i be verified by evidence j?\n",
        "  # ('Partially'/ 'Unsupported' / 'Fully' / 'Contradictory')\n",
        "  # Does not apply to logic steps.\n",
        "\n",
        "  print(f\"- Attribution Justification: {step_info[i]['attribution_justifications']}\\n\")\n",
        "\n",
        "  # If this is the final step, print answer-level labels\n",
        "  if i == len(step_info) - 1:\n",
        "    print('-----')\n",
        "    print(\"=== FINAL ANSWER ANNOTATIONS ===\")\n",
        "    print(f\"- Fully Attributable: {step_info[i]['answer_is_fully_attributable']}\")\n",
        "    # Is the final answer fully attributable? (True / False)\n",
        "\n",
        "    print(f\"- Logically Correct: {step_info[i]['answer_is_logically_correct']}\")\n",
        "    # Is the answer fully logically correct? (True / False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn30geWTvxUx"
      },
      "source": [
        "## Set up OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5wiuOdq3w99L"
      },
      "outputs": [],
      "source": [
        "def query_llm_as_judge(prompt, task, client, prompts, model_family = 'openai', model_type = 'gpt-3.5-turbo'):\n",
        "    res = None\n",
        "    model_family = model_family.lower()\n",
        "    system_prompt = prompts[task]\n",
        "    if model_family == 'openai':\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_type,\n",
        "            messages= [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        res = response.choices[0].message.content.strip()\n",
        "\n",
        "    elif model_family == 'anthropic':\n",
        "        response = client.messages.create(\n",
        "            model=model_type,\n",
        "            max_tokens=1000,\n",
        "            system=system_prompt,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        res = response.content[0].text\n",
        "\n",
        "    elif model_family == 'gemini':\n",
        "        response = client.models.generate_content(\n",
        "            model=model_type,\n",
        "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
        "            contents=prompt\n",
        "        )\n",
        "        res =  response.text\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Z_PeUzUE3u"
      },
      "source": [
        "## Evaluate REVEAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nFKI2BWS2EaM"
      },
      "outputs": [],
      "source": [
        "system_prompts = {\n",
        "    \"type_label\":\n",
        "      (\"You are an expert at analyzing each step in a multi-step answer. Given a step used to answer a question,\"\n",
        "      \"your job is to decide whether it contains factual information that must be verified by external sources\"\n",
        "      \"(Attribution step.), performs logical inference from past steps (Logical step.), or does both (Both.). \"\n",
        "      \"Only output the answer, without explanation.\"),\n",
        "    \"correctness_label\":\n",
        "      (\n",
        "          \"You are a logic expert. Your job is to determine if a reasoning step (the hypothesis) follows logically \"\n",
        "          \"from the previous steps (the premise) and the question. You can assume all statements are correct \"\n",
        "          \"in previous steps. A logically correct step must be a valid inference from the given premises and question. \"\n",
        "          \"If it is a valid logical inference, output 'Correct'. If it does not, output 'Incorrect'. \"\n",
        "          \"Only output the answer, without explanation.\\n\\n\"\n",
        "          \"Here are some examples:\\n\\n\"\n",
        "          \"Human:\\n\"\n",
        "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
        "          \"Premise: 'France and Japan are separated by thousands of kilometers and an ocean.'\\n\"\n",
        "          \"Hypothesis: 'No, you cannot walk from France to Japan.'\\n\"\n",
        "          \"Output: {{Correct, Incorrect}}\"\n",
        "          \"Assistant:\\n\"\n",
        "          \"Correct\\n\\n\"\n",
        "          \"Human:\\n\"\n",
        "          \"Question: 'Is this statement plausible: John threw a strike out at Little League on Sunday.'\\n\"\n",
        "          \"Premise: 'Little League is a youth baseball organization. Striking out is a common baseball term.'\\n\"\n",
        "          \"Hypothesis: 'No, the statement is not plausible.'\\n\"\n",
        "          \"Output: {{Correct, Incorrect}}\"\n",
        "          \"Assistant:\\n\"\n",
        "          \"Incorrect\"\n",
        "      ),\n",
        "    \"logic_relevance_label\":\n",
        "       (\n",
        "          \"You are a judge evaluating whether each step in a multi-step answer contributes \"\n",
        "          \"meaningfully toward answering the question. Steps that are off-topic or irrelevant \"\n",
        "          \"should be marked 'Not Relevant'. Others should be marked 'Relevant'. Only output the answer, without explanation.\"\n",
        "          \"Here are some examples:\\n\\n\"\n",
        "          \"Human:\\n\"\n",
        "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
        "          \"Answer: 'France and Japan are separated by thousands of kilometers and an ocean. France has more walking areas than Japan. No, you cannot walk from France to Japan.'\\n\"\n",
        "          \"Step: France has more walking areas than Japan.\"\n",
        "          \"Is this step relevant answering the question? Output: {{Relevant, Not Relevant}}\"\n",
        "          \"Assistant:\\n\"\n",
        "          \"Not Relevant\\n\\n\"\n",
        "          \"Human:\\n\"\n",
        "          \"Question: 'Is this statement plausible: John threw a strike out at Little League on Sunday.'\\n\"\n",
        "          \"Answer: 'Little League is a youth baseball organization. Striking out is a common baseball term. No, the statement is not plausible.'\\n\"\n",
        "          \"Step: Striking out is a common baseball term.\"\n",
        "          \"Is this step relevant to answering the question? Output: {{Relevant, Not Relevant}}\"\n",
        "          \"Assistant:\\n\"\n",
        "          \"Relevant\\n\\n\"\n",
        "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
        "          \"Answer: 'France and Japan are separated by thousands of kilometers and an ocean. France has more walking areas than Japan. So the answer is No.'\\n\"\n",
        "          \"Step: So the answer is No.\"\n",
        "          \"Is this step relevant to answering the quesiton? Output: {{Relevant, Not Relevant}}\"\n",
        "          \"Assistant:\\n\"\n",
        "          \"Relevant\\n\\n\"\n",
        "        ),\n",
        "    \"logic_relevance_label_reflect\": (\n",
        "         \"You are a logic expert reviewing your own reasoning. You must evaluate whether each step in a multi-step answer contributes \"\n",
        "        \"meaningfully toward answering the question. Steps that are off-topic, redundant, or irrelevant \"\n",
        "        \"should be marked 'Not Relevant'. Others should be marked 'Relevant'. You previously said this step is relevant. \"\n",
        "        \"I want you to reflect carefully: Is this step relevant to answering the question? Briefly explain your answer then \"\n",
        "        \"at the end, output your final judgment clearly as either: Relevant or Not Relevant.\"\n",
        "    ),\n",
        "    \"answer_is_logically_correct\": (\n",
        "        \"You are a fact-checking and logic expert. Given a question, a multi-step answer, and supporting evidence, \"\n",
        "        \"determine whether the answer is fully correct both logically and factually. Refer to the provided evidence, if relevant. \"\n",
        "        \"Output 'True' if correct or 'False' if incorrect. Only output the answer, without explanation.\"\n",
        "    ),\n",
        "    \"correctness_label_reflect\": (\n",
        "        \"You are a logic expert reviewing your own reasoning. You claimed that the following reasoning step logically follows \"\n",
        "        \"from earlier steps (the premise) and the question being asked. \"\n",
        "        \"However, you may be wrong, and people are doubting your reasoning. I want you to reflect carefully: \"\n",
        "        \"Does the hypothesis follow logically from the question and the premise? Assume all earlier steps are factually correct.\"\n",
        "        \"Defend your reasoning if it is valid. Otherwise, admit the error. Don't overthink. \"\n",
        "        \"At the end, output your final judgment clearly as either: Correct or Incorrect.\"\n",
        "    ),\n",
        "    \"correctness_label_reflect_confidence\": (\n",
        "        \"You are assessing the validity of your own logical step. Given the question and the preceding, \"\n",
        "        \"factually sound steps (the premise), you previously stated that the following step logically follows. \"\n",
        "        \"Now, critically evaluate the strength of this logical connection. On a scale of 1 to 10 (where 1 is \"\n",
        "        \"'not at all' and 10 is 'absolutely certain'), how confident are you that the hypothesis logically follows? Is it \"\n",
        "        \"a valid inference and is it doing everything correct? Explain your confidence score in 1-3 sentences. Finally, state your overall judgment: Correct or Incorrect.\"\n",
        "    ),\n",
        "    \"correctness_label_reflect_law\": (\n",
        "        \"You are defending yourself in court regarding a statement you made. You claimed that the following reasoning step logically follows \"\n",
        "        \"from earlier steps (the premise) and the question being asked. \"\n",
        "        \"Assuming that all earlier steps are factually correct, is the inference you made in this step correct?\"\n",
        "        \"Provide your defense. Remember, you cannot lie on the stand. Defend your reasoning if it is valid. Otherwise, admit the error. \"\n",
        "        \"Don't overthink. At the end, output your final judgment to the jury clearly as either: Correct or Incorrect. \"\n",
        "    ),\n",
        "    \"alignment\": (\n",
        "        \"You are a judge evaluating whether a set of steps in a multi-step answer is in alignment \"\n",
        "        \"with the answer to the question. Outputs where the answer is not derived from the steps should be marked \"\n",
        "        \"as No, and outputs where the answer does come in alignment with the steps should be marked as Yes. Only output the answer \"\n",
        "        \"without explanation.\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qeVEKeeq1ThQ"
      },
      "outputs": [],
      "source": [
        "def step_type_prompt(question, previous_steps, step):\n",
        "    return f\"\"\"Step Type Task\n",
        "            Question: {question}\n",
        "            Past Steps: {previous_steps}\n",
        "            Step: {step}\n",
        "            Step type: {{Attribution step., Logical step., Both.}}\"\"\"\n",
        "\n",
        "def logic_correctness_prompt(question, previous_steps, step):\n",
        "    return f\"\"\"Logic Task\n",
        "            Question: '{question}'\n",
        "            Premise: '{previous_steps}'\n",
        "            Is the following hypothesis a correct logical inference based on the premise and question being asked?\n",
        "            Hypothesis: '{step}'\n",
        "            Output: {{Correct, Incorrect}}\"\"\"\n",
        "\n",
        "def logic_correctness_reflect_prompt(question, previous_steps, step):\n",
        "    return f\"\"\"Self-Critique Logic Task\n",
        "            Question: '{question}'\n",
        "            Premise: '{previous_steps}'\n",
        "\n",
        "            Is the following a correct logical inference based on the premise and question being asked?\n",
        "            You can use your external knowledge as long as it doesn't contradict the premise.\n",
        "            Hypothesis: '{step}'\n",
        "\n",
        "            Briefly explain your answer then clearly state your output in a new line in this format:\n",
        "            Output: {{Correct, Incorrect}}\"\"\"\n",
        "\n",
        "def relevance_prompt(question, cot_answer, step):\n",
        "    return f\"\"\"Relevance Task\n",
        "            Question: {question}\n",
        "            Answer: {cot_answer}\n",
        "            Step: {step}\n",
        "            Is this step relevant to answering the question?\n",
        "            Output: {{Relevant, Not Relevant}}\"\"\"\n",
        "\n",
        "def relevance_reflect_prompt(question, cot_answer, step):\n",
        "    return f\"\"\"Self-Critique Relevance Task\n",
        "            Question: '{question}'\n",
        "            Answer: '{cot_answer}'\n",
        "\n",
        "            Is the following step relevant to the answer?\n",
        "            Step: '{step}'\n",
        "\n",
        "            Briefly explain your answer then clearly state your output in a new line in this format:\n",
        "            Output: {{Relevant, Not Relevant}}\"\"\"\n",
        "\n",
        "def cot_correctness_prompt(evidence_list, question, cot_answer):\n",
        "    # Create formatted list of evidence blocks\n",
        "    evidence_section = \"\\n\".join([f\"Evidence {i+1}: {e}\" for i, e in enumerate(evidence_list)])\n",
        "    return f\"\"\"CoT Correctness task\n",
        "              {evidence_section}\n",
        "              Question: {question}\n",
        "              Answer: {cot_answer}\n",
        "              The answer is: {{True, False}}\"\"\"\n",
        "\n",
        "def reasoning_answer_alignment_prompt(question, reasoning_steps, answer):\n",
        "    return f\"\"\"Reasoning Task\n",
        "            Question: '{question}'\n",
        "            Reasoning steps: '{reasoning_steps}'\n",
        "            Answer: '{answer}'\n",
        "\n",
        "            Are the reasoning steps in alignment with the answer?\n",
        "            Output: {{Yes, No}}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "enqRV28o4DQj"
      },
      "outputs": [],
      "source": [
        "def extract_answer_from_response(text, output_type='correctness'):\n",
        "    if output_type == 'correctness':\n",
        "        match = re.search(r\"Output:\\s*(Correct|Incorrect)\", text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).capitalize()  # Returns 'Correct' or 'Incorrect'\n",
        "    elif output_type == 'relevance':\n",
        "        match = re.search(r\"Output:\\s*(Relevant|Not Relevant)\", text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).capitalize()  # Returns 'Relevant' or 'Not Relevant'\n",
        "    return None\n",
        "\n",
        "def evaluate_cot_steps_ensemble(grouped_data, clients, model_types):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        grouped_data: dict with (question_id, answer_id, answer_model) -> steps\n",
        "        clients: dict of {model_family: client}\n",
        "        model_types: dict of {model_family: model_name}\n",
        "    Returns:\n",
        "        DataFrame with step-wise and final predictions using majority vote\n",
        "    \"\"\"\n",
        "\n",
        "    def majority_vote(preds):\n",
        "        # strip all punctuation including new lines and make lowercase\n",
        "        preds = [v.lower() for v in preds]\n",
        "        preds = [re.sub(r'[^\\w\\s]', '', v) for v in preds]\n",
        "        tally = collections.Counter(preds)\n",
        "\n",
        "        if len(tally) == 0:\n",
        "            return None\n",
        "        return tally.most_common(1)[0][0]  # Most frequent prediction\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, ((question_id, answer_id, answer_model), steps) in enumerate(grouped_data.items()):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processing {i}/{len(grouped_data)}\")\n",
        "\n",
        "        question = steps[0]['question']\n",
        "        full_cot = \" \".join([s['step'] for s in steps])\n",
        "        prev_steps = \"\"\n",
        "        evidence_list = []\n",
        "\n",
        "        for i, step in enumerate(steps):\n",
        "            step_text = step['step']\n",
        "            step_type = step['type_label']\n",
        "\n",
        "            # Step Type: use ground truth for now\n",
        "            normalized_type = step_type.strip().title()\n",
        "            logic_correctness_preds = {}\n",
        "\n",
        "            # Logical Correctness Task: evaluate whenever the actual df has a correctness label\n",
        "            if \"Logic\" in normalized_type and not pd.isna(step.get(\"correctness_label\")):\n",
        "                logic_prompt_text = logic_correctness_prompt(question, prev_steps, step_text)\n",
        "                for model_family, client in clients.items():\n",
        "                    model_type = model_types[model_family]\n",
        "                    logic_output = query_llm_as_judge(logic_prompt_text, \"correctness_label\", client, system_prompts, model_family, model_type)\n",
        "                    # print(logic_prompt_text, logic_output)\n",
        "                    # logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\n",
        "                    logic_output = logic_output.replace(\"\\n\", \"\")\n",
        "                    if logic_output:\n",
        "                        logic_correctness_preds[model_family] = logic_output\n",
        "\n",
        "                correctness = majority_vote(list(logic_correctness_preds.values()))\n",
        "                if correctness.lower() != step.get(\"correctness_label\").lower():\n",
        "                    print(logic_prompt_text, logic_correctness_preds, f'Correct Answer: {step.get(\"correctness_label\")}')\n",
        "            else:\n",
        "                correctness = None\n",
        "\n",
        "            # Relevance Task\n",
        "            relevance_preds = {}\n",
        "            relevance_prompt_text = relevance_prompt(question, full_cot, step_text)\n",
        "            for model_family, client in clients.items():\n",
        "                model_type = model_types[model_family]\n",
        "                relevance_output = query_llm_as_judge(relevance_prompt_text, \"logic_relevance_label\", client, system_prompts, model_family, model_type)\n",
        "                # relevance_output = extract_answer_from_response(relevance_output, 'relevance')  # only if using self reflection prompting\n",
        "                relevance_output = relevance_output.replace(\"\\n\", \"\")\n",
        "                if relevance_output:\n",
        "                    relevance_preds[model_family] = relevance_output\n",
        "\n",
        "            relevance = majority_vote(list(relevance_preds.values()))\n",
        "            if relevance.lower() != step.get(\"logic_relevance_label\").lower():\n",
        "                print(relevance_prompt_text, relevance_preds, f'Correct Answer: {step.get(\"logic_relevance_label\")}')\n",
        "\n",
        "            # Get Evidence\n",
        "            if \"Attribution\" in normalized_type and step.get(\"evidence\"):\n",
        "                evidence_list.append(step['evidence'])\n",
        "\n",
        "            if i == len(steps) - 1:\n",
        "                reasoning_steps = prev_steps\n",
        "                answer = step_text\n",
        "            prev_steps += f\"{step_text} \"\n",
        "\n",
        "            result_entry = {\n",
        "                \"question_id\": question_id,\n",
        "                \"question\": step['question'],\n",
        "                \"answer_model\": answer_model,\n",
        "                \"answer_id\": answer_id,\n",
        "                \"step_idx\": step['step_idx'],\n",
        "                \"step\": step_text,\n",
        "                \"type_label_pred\": step_type,\n",
        "                \"correctness_label_pred\": correctness,\n",
        "                \"logic_relevance_label_pred\": relevance,\n",
        "                \"type_label_true\": step.get(\"type_label\"),\n",
        "                \"correctness_label_true\": step.get(\"correctness_label\"),\n",
        "                \"logic_relevance_label_true\": step.get(\"logic_relevance_label\"),\n",
        "            }\n",
        "\n",
        "            # Add individual model verdicts\n",
        "            for model_family in clients.keys():\n",
        "                result_entry[f\"correctness_label_{model_family}\"] = logic_correctness_preds.get(model_family)\n",
        "                result_entry[f\"logic_relevance_label_{model_family}\"] = relevance_preds.get(model_family)\n",
        "\n",
        "            results.append(result_entry)\n",
        "\n",
        "        ###################### METRIC FOR REASONING ALIGNMENT ############################\n",
        "        # print(prev_steps, reasoning_steps, answer)\n",
        "        reasoning_alignment_prompt = reasoning_answer_alignment_prompt(question, reasoning_steps, answer)\n",
        "        alignment_preds = {}\n",
        "        for model_family, client in clients.items():\n",
        "            model_type = model_types[model_family]\n",
        "            alignment_output = query_llm_as_judge(reasoning_alignment_prompt, \"alignment\", client, system_prompts, model_family, model_type)\n",
        "\n",
        "            # logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\n",
        "            alignment_output = alignment_output.replace(\"\\n\", \"\")\n",
        "            if alignment_output:\n",
        "                alignment_preds[model_family] = alignment_output\n",
        "\n",
        "        alignment_verdict = majority_vote(list(alignment_preds.values()))\n",
        "        results[-1][\"answer_is_in_alignment_pred\"] = alignment_verdict\n",
        "        # print(reasoning_alignment_prompt, alignment_preds)\n",
        "\n",
        "        # TODO: we don't have annotations for alignment\n",
        "        # if correctness.lower() != step.get(\"correctness_label\").lower():\n",
        "        #     print(logic_prompt_text, logic_correctness_preds, f'Correct Answer: {step.get(\"correctness_label\")}')\n",
        "\n",
        "        ###################### METRIC FOR REASONING ALIGNMENT ############################\n",
        "\n",
        "        # Final CoT Correctness: use placeholder for now\n",
        "        cot_correctness = 'True'\n",
        "        results[-1][\"answer_is_logically_correct_pred\"] = cot_correctness\n",
        "        results[-1][\"answer_is_logically_correct_true\"] = steps[-1].get(\"answer_is_logically_correct\")\n",
        "\n",
        "        # break # TODO: remove break, it is only for printing purposes\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO8X9c6L4oNF",
        "outputId": "cac951d2-41c7-4a56-c8ab-99464a204ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 0/3\n"
          ]
        }
      ],
      "source": [
        "# API Keys\n",
        "MY_OPENAI_KEY = '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'\n",
        "MY_ANTHROPIC_KEY = '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'\n",
        "MY_GEMINI_KEY = 'AIzaSyC13qSGNQ8vMeqwkQdQA1pQ7o4LSBZJBX0'\n",
        "\n",
        "# Connect to APIs\n",
        "gpt_client = OpenAI(api_key = MY_OPENAI_KEY)\n",
        "claude_client = anthropic.Anthropic(api_key = MY_ANTHROPIC_KEY)\n",
        "gemini_client = genai.Client(api_key=MY_GEMINI_KEY)\n",
        "\n",
        "# TODO: Change clients_dict and models_dict based on the LLMs you want to evaluate\n",
        "# TODO: If no majority vote ensembling, just choose one model\n",
        "clients_dict = {'openai': gpt_client, 'anthropic': claude_client, 'gemini': gemini_client}\n",
        "models_dict = {'openai': 'gpt-3.5-turbo', 'anthropic': 'claude-3-haiku-20240307', 'gemini': 'gemini-1.5-flash'}\n",
        "\n",
        "# Make a subset of grouped_data with n data points to test on, set random seed so results are comparable\n",
        "random.seed(42)\n",
        "n = 3\n",
        "subset_keys = random.sample(list(grouped_data.keys()), n)\n",
        "subset_data = {k: grouped_data[k] for k in subset_keys}\n",
        "\n",
        "# Ensemble evaluation\n",
        "results = evaluate_cot_steps_ensemble(subset_data, clients_dict, models_dict)\n",
        "\n",
        "# Format results to lowercase and remove punctuation\n",
        "results = results.map(lambda x: str(x).lower().replace('.', '').replace('\\t', ' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckw36GdCo-do",
        "outputId": "a36a6713-fe27-450d-df34-1a030cdca2b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type_label_pred: ['attribution step' 'logical step']\n",
            "correctness_label_pred: ['none' 'correct']\n",
            "logic_relevance_label_pred: ['relevant']\n",
            "type_label_true: ['attribution step' 'logical step']\n",
            "correctness_label_true: ['nan' 'correct']\n",
            "logic_relevance_label_true: ['relevant']\n",
            "answer_is_in_alignment_pred: ['nan' 'yes']\n",
            "answer_is_logically_correct_pred: ['nan' 'true']\n",
            "answer_is_logically_correct_true: ['nan' 'true']\n"
          ]
        }
      ],
      "source": [
        "# Print unique values in all columns to check for formatting issues in LLM responses\n",
        "for col in results.columns:\n",
        "    if 'pred' in col or 'true' in col:\n",
        "      print(f\"{col}: {results[col].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sR3gkV-J5S0S"
      },
      "outputs": [],
      "source": [
        "def evaluate_predictions(df, task_column_prefix, is_ensemble=False):\n",
        "    if not is_ensemble:\n",
        "        y_true = df[f\"{task_column_prefix}_true\"].astype(str).str.strip().str.title()\n",
        "        y_pred = df[f\"{task_column_prefix}_pred\"].astype(str).str.strip().str.title()\n",
        "\n",
        "        report = classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "        return report\n",
        "    else:\n",
        "        y_true = df[f\"{task_column_prefix}_true\"].astype(str).str.strip().str.title()\n",
        "        y_pred_ensemble = df[f\"{task_column_prefix}_pred\"].astype(str).str.strip().str.title()\n",
        "        y_pred_openai = df[f\"{task_column_prefix}_openai\"].astype(str).str.strip().str.title()\n",
        "        y_pred_anthropic = df[f\"{task_column_prefix}_anthropic\"].astype(str).str.strip().str.title()\n",
        "        y_pred_gemini = df[f\"{task_column_prefix}_gemini\"].astype(str).str.strip().str.title()\n",
        "\n",
        "        reports = {}\n",
        "        reports['ensemble'] = classification_report(y_true, y_pred_ensemble, output_dict=True, zero_division=0)\n",
        "        reports['openai'] = classification_report(y_true, y_pred_openai, output_dict=True, zero_division=0)\n",
        "        reports['anthropic'] = classification_report(y_true, y_pred_anthropic, output_dict=True, zero_division=0)\n",
        "        reports['gemini'] = classification_report(y_true, y_pred_gemini, output_dict=True, zero_division=0)\n",
        "\n",
        "        return reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu1MyGgO6_0F",
        "outputId": "e3fe1e7f-827a-43cc-aa84-241adfe638f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro F1 score for logical step correctness (ensemble): 1.0\n",
            "Macro F1 score for step relevance (ensemble): 1.0\n",
            "Weighted F1 score for logical step correctness (ensemble): 1.0\n",
            "Weighted F1 score for step relevance (ensemble): 1.0\n",
            "------\n",
            "Macro F1 score for logical step correctness (openai): 0.42857142857142855\n",
            "Macro F1 score for step relevance (openai): 1.0\n",
            "Weighted F1 score for logical step correctness (openai): 0.8571428571428571\n",
            "Weighted F1 score for step relevance (openai): 1.0\n",
            "------\n",
            "Macro F1 score for logical step correctness (anthropic): 1.0\n",
            "Macro F1 score for step relevance (anthropic): 1.0\n",
            "Weighted F1 score for logical step correctness (anthropic): 1.0\n",
            "Weighted F1 score for step relevance (anthropic): 1.0\n",
            "------\n",
            "Macro F1 score for logical step correctness (gemini): 1.0\n",
            "Macro F1 score for step relevance (gemini): 1.0\n",
            "Weighted F1 score for logical step correctness (gemini): 1.0\n",
            "Weighted F1 score for step relevance (gemini): 1.0\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "# MACRO F1 SCORES (same as what is used in paper, unweighted mean, better for class imbalances)\n",
        "logic_metrics = evaluate_predictions(results[results['correctness_label_true'] != 'nan'], \"correctness_label\", is_ensemble = True)\n",
        "relevance_metrics = evaluate_predictions(results, \"logic_relevance_label\", is_ensemble = True)\n",
        "\n",
        "for model, metrics in logic_metrics.items():\n",
        "    print(f\"Macro F1 score for logical step correctness ({model}):\", metrics['macro avg']['f1-score'])\n",
        "    print(f\"Macro F1 score for step relevance ({model}):\", relevance_metrics[model]['macro avg']['f1-score'])\n",
        "    print(f\"Weighted F1 score for logical step correctness ({model}):\", metrics['weighted avg']['f1-score'])\n",
        "    print(f\"Weighted F1 score for step relevance ({model}):\", relevance_metrics[model]['weighted avg']['f1-score'])\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPYWTZMlUgva"
      },
      "source": [
        "## External Validation: BigBenchMistake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Z2t4XiZOYnY1"
      },
      "outputs": [],
      "source": [
        "df_bbm = pd.read_json('data/logical_deduction.jsonl', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "l4NKPpNxUjka"
      },
      "outputs": [],
      "source": [
        "system_prompts_bbm = {\n",
        "    \"mistake_label\":\n",
        "      (\n",
        "          \"You are a logic expert. Your job is to determine the first incorrect reasoning step in a chain of thought answer.\"\n",
        "          \"If the given step has a mistake, output Yes. Otherwise output No. A mistkae can be a step that is logically incorrect or requires big \"\n",
        "          \"logical jumps based on prior steps, a step that is redundant or repetetive, off topic or containing gibberish, leading to a dead \"\n",
        "          \"end, an incorrect final answer, or referring to external links, images, or graphs. \"\n",
        "          \"Only output the answer, without explanation.\\n\\n\"\n",
        "          \"Here is an example:\\n\\n\"\n",
        "          \"\"\"\\\n",
        "            Question: The following statements each describe a set of five objects arranged in a fixed order. The statements are logically consistent.\n",
        "            - On a shelf, there are five books: a red book, a gray book, a white book, a blue book, and a green book.\n",
        "            - The white book is to the left of the gray book.\n",
        "            - The gray book is the third from the left.\n",
        "            - The red book is the second from the left.\n",
        "            - The blue book is to the right of the green book.\n",
        "            Given the above statements, which of the following is correct?\n",
        "            Options:\n",
        "            (A) The red book is the leftmost\n",
        "            (B) The gray book is the leftmost\n",
        "            (C) The white book is the leftmost\n",
        "            (D) The blue book is the leftmost\n",
        "            (E) The green book is the leftmost\n",
        "\n",
        "            Previous Steps:\n",
        "            To answer the question, I need to identify the positions of all five books. I should first find and look through any statements about absolute positions of books. Then, if there are remaining gaps, I can look through statements about relative positions and find the final order by process of elimination.\n",
        "            The gray book is third from the left. So from left to right: first and second can be any book, then third is the gray book, then fourth and fifth can be any book.\n",
        "\n",
        "            Current Step:\n",
        "            The white book is to the left of the gray book. We previously concluded that the gray book is third from the left, so the white book must be second from the left.\n",
        "\n",
        "            Is there a mistake in the above step? Output Yes if there is a mistake. Otherwise output No.\n",
        "            (Note to assistant: In this example, there is a mistake becasue we cannot assume that the white book is second to the left based only on the fact that it is to the left of the gray book, it can also be first to the left.)\n",
        "            Assistant: Yes \"\"\"\n",
        "      )\n",
        "}\n",
        "\n",
        "\n",
        "def mistake_prompt(question, steps, step):\n",
        "    return f\"\"\"Mistake Finding Task:\n",
        "            Question: {question}\n",
        "\n",
        "            Previous Steps:\n",
        "            {steps}\n",
        "\n",
        "            Current Step:\n",
        "            {step}\n",
        "\n",
        "            Is there a mistake in the above step? Output Yes if there is a mistake. Otherwise output No. Output only the answer as a Yes/No on a new line, with nothing else.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qM-LY28BUtSP"
      },
      "outputs": [],
      "source": [
        "def evaluate_cot_steps_ensemble(grouped_data, clients, model_types):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        grouped_data: dict with (question_id, answer_id, answer_model) -> steps\n",
        "        clients: dict of {model_family: client}\n",
        "        model_types: dict of {model_family: model_name}\n",
        "    Returns:\n",
        "        DataFrame with step-wise and final predictions using majority vote\n",
        "    \"\"\"\n",
        "\n",
        "    def majority_vote(preds):\n",
        "        # strip all punctuation including new lines and make lowercase\n",
        "        preds = [v.lower() for v in preds]\n",
        "        preds = [re.sub(r'[^\\w\\s]', '', v) for v in preds]\n",
        "        tally = collections.Counter(preds)\n",
        "        if len(tally) == 0:\n",
        "            return None\n",
        "        return tally.most_common(1)[0][0]  # Most frequent prediction\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, ((question, answer), steps) in enumerate(grouped_data.items()):\n",
        "        if pd.isna(answer):\n",
        "            continue\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processing {i}/{len(grouped_data)}\")\n",
        "\n",
        "        assert len(steps) == 1\n",
        "\n",
        "        pred = \"No\"\n",
        "        all_steps = steps[0]\n",
        "        for i, step in enumerate(all_steps[1:]):\n",
        "            step_num = i + 1\n",
        "            # use ground truth for now\n",
        "            mistake_preds = {}\n",
        "            first_mistake_preds = {}\n",
        "            full_cot = \"\\n\".join(all_steps[1:i])\n",
        "\n",
        "            mistake_prompt_text = mistake_prompt(question, full_cot, step)\n",
        "            for model_family, client in clients.items():\n",
        "                model_type = model_types[model_family]\n",
        "                output = query_llm_as_judge(mistake_prompt_text, \"mistake_label\", client, system_prompts_bbm, model_family, model_type)\n",
        "                output = output.replace(\"\\n\", \"\")\n",
        "                if output.lower() == \"yes\" and model_family not in first_mistake_preds:\n",
        "                    first_mistake_preds[model_family] = step_num\n",
        "                else:\n",
        "                    first_mistake_preds[model_family] = \"no\"\n",
        "                mistake_preds[model_family] = output\n",
        "\n",
        "            ensemble_output = majority_vote(list(mistake_preds.values()))\n",
        "\n",
        "            if ensemble_output == \"yes\":\n",
        "                pred = step_num\n",
        "                break\n",
        "\n",
        "            if answer == step_num:\n",
        "                break\n",
        "\n",
        "        if pred != answer:\n",
        "            print(mistake_prompt_text, first_mistake_preds, f'Prediction: {pred}', f'Correct Answer: {answer}')\n",
        "\n",
        "        if isinstance(answer, (float, int)):\n",
        "            answer = int(answer)\n",
        "\n",
        "        result_entry = {\n",
        "            \"question\": question,\n",
        "            \"steps\": full_cot,\n",
        "            \"mistake_index_true\": answer,\n",
        "            \"mistake_index_pred\": pred\n",
        "        }\n",
        "\n",
        "        # Add individual model verdicts\n",
        "        for model_family in clients.keys():\n",
        "            result_entry[f\"mistake_index_{model_family}\"] = first_mistake_preds.get(model_family)\n",
        "\n",
        "        results.append(result_entry)\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "D5cAdAauUwS7"
      },
      "outputs": [],
      "source": [
        "grouped_data_bbm = defaultdict(list)\n",
        "for i, row in df_bbm.iterrows():\n",
        "    grouped_data_bbm[(row['input'], row['mistake_index'])].append(row['steps'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECg269wYUxgX",
        "outputId": "cddcbec3-a181-4a20-e100-362d876aa918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 0/3\n",
            "Mistake Finding Task:\n",
            "            Question: The following statements each describe a set of three objects arranged in a fixed order. The statements are logically consistent.\n",
            "- On a shelf, there are three books: a red book, a blue book, and an orange book.\n",
            "- The red book is to the left of the orange book.\n",
            "- The orange book is the second from the left.\n",
            "Q: Given the above statements, which of the following is correct?\n",
            "Options:\n",
            "(A) The red book is the leftmost.\n",
            "(B) The blue book is the leftmost.\n",
            "(C) The orange book is the leftmost.\n",
            "\n",
            "            Previous Steps:\n",
            "            \n",
            "\n",
            "            Current Step:\n",
            "            The orange book is the second from the left, so the blue book must be the leftmost.\n",
            "\n",
            "            Is there a mistake in the above step? Output Yes if there is a mistake. Otherwise output No. Output only the answer as a Yes/No on a new line, with nothing else. {'openai': 'no', 'anthropic': 'no', 'gemini': 1} Prediction: No Correct Answer: 1.0\n",
            "Mistake Finding Task:\n",
            "            Question: The following statements each describe a set of five objects arranged in a fixed order. The statements are logically consistent.\n",
            "- In an antique car show, there are five vehicles: a hatchback, a bus, a convertible, a tractor, and a minivan.\n",
            "- The tractor is older than the bus.\n",
            "- The minivan is newer than the bus.\n",
            "- The hatchback is the second-newest.\n",
            "- The minivan is older than the convertible.\n",
            "Q: Given the above statements, which of the following is correct?\n",
            "Options:\n",
            "(A) The hatchback is the third-newest.\n",
            "(B) The bus is the third-newest.\n",
            "(C) The convertible is the third-newest.\n",
            "(D) The tractor is the third-newest.\n",
            "(E) The minivan is the third-newest.\n",
            "\n",
            "            Previous Steps:\n",
            "            \n",
            "\n",
            "            Current Step:\n",
            "            The hatchback is the second-newest. So from oldest to newest: first can be any vehicle, then second is the hatchback, then third can be any vehicle, then fourth can be any vehicle, then fifth is the newest vehicle.\n",
            "\n",
            "            Is there a mistake in the above step? Output Yes if there is a mistake. Otherwise output No. Output only the answer as a Yes/No on a new line, with nothing else. {'openai': 'no', 'anthropic': 'no', 'gemini': 1} Prediction: No Correct Answer: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Make a subset of grouped_data with n data points to test on, set random seed so results are comparable\n",
        "random.seed(42)\n",
        "n = 3\n",
        "subset_keys_bbm = random.sample(list(grouped_data_bbm.keys()), n)\n",
        "subset_data_bbm = {k: grouped_data_bbm[k] for k in subset_keys_bbm}\n",
        "\n",
        "# Ensemble evaluation\n",
        "results = evaluate_cot_steps_ensemble(subset_data_bbm, clients_dict, models_dict)\n",
        "\n",
        "# Format results to lowercase and remove punctuation\n",
        "results = results.map(lambda x: str(x).lower().replace('.', '').replace('\\t', ' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISDHXK8U16Q",
        "outputId": "a80e6273-dfff-4c42-b0d4-effef9c9415b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mistake_index_true: ['1' '3']\n",
            "mistake_index_pred: ['no' '3']\n"
          ]
        }
      ],
      "source": [
        "# Print unique values in all columns to check for formatting issues in LLM responses\n",
        "for col in results.columns:\n",
        "    if 'pred' in col or 'true' in col:\n",
        "      print(f\"{col}: {results[col].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGJQffkdU_Wh",
        "outputId": "12e3bbc8-0a5e-42fd-8d9d-838dfddc0d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro F1 score for logical step correctness (ensemble): 0.3333333333333333\n",
            "Weighted F1 score for logical step correctness (ensemble): 0.3333333333333333\n",
            "Accuracy (ensemble): 0.3333333333333333\n",
            "------\n",
            "Macro F1 score for logical step correctness (openai): 0.0\n",
            "Weighted F1 score for logical step correctness (openai): 0.0\n",
            "Accuracy (openai): 0.0\n",
            "------\n",
            "Macro F1 score for logical step correctness (anthropic): 0.3333333333333333\n",
            "Weighted F1 score for logical step correctness (anthropic): 0.3333333333333333\n",
            "Accuracy (anthropic): 0.3333333333333333\n",
            "------\n",
            "Macro F1 score for logical step correctness (gemini): 1.0\n",
            "Weighted F1 score for logical step correctness (gemini): 1.0\n",
            "Accuracy (gemini): 1.0\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "# MACRO F1 SCORES (same as what is used in paper, unweighted mean, better for class imbalances)\n",
        "mistake_metrics = evaluate_predictions(results, \"mistake_index\", is_ensemble = True)\n",
        "\n",
        "for model, metrics in mistake_metrics.items():\n",
        "    print(f\"Macro F1 score for logical step correctness ({model}):\", metrics['macro avg']['f1-score'])\n",
        "    print(f\"Weighted F1 score for logical step correctness ({model}):\", metrics['weighted avg']['f1-score'])\n",
        "    print(f\"Accuracy ({model}):\", metrics['accuracy'])\n",
        "    print(\"------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
