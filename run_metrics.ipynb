{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe903ea6",
   "metadata": {},
   "source": [
    "# Calculate Metrics on BIG-Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d248604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import anthropic\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import re\n",
    "import collections\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757f2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Keith is 5 feet tall so he is less likely to become an amateur basketball player than a horse jockey.\n",
      "Prompt CoT: Evaluate if the following Q follows common sense. Answer 'True' or 'False'\n",
      "Q: Keith is 5 feet tall so he is less likely to become an amateur basketball player than a horse jockey.\n",
      "In answering this question each step should be on a separate line and start with a number and a period, followed by the reasoning. Finally the answer should be on a new line with the word 'Answer' proceeded by a colon.\n",
      "A: Let's think step by step.\n",
      "1200\n",
      "dict_keys(['id', 'task', 'question', 'answer', 'prompt_direct', 'prompt_cot', 'response', 'steps'])\n"
     ]
    }
   ],
   "source": [
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'data/big_bench_augmented_with_steps.json'\n",
    "data = load_json_file(file_path)\n",
    "\n",
    "if data:\n",
    "    print('Question:', data[0]['question'])\n",
    "    print('Prompt CoT:', data[0]['prompt_cot'])\n",
    "    print(len(data))\n",
    "    print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53371790",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = {\n",
    "    \"type_label\":\n",
    "      (\"You are an expert at analyzing each step in a multi-step answer. Given a step used to answer a question,\"\n",
    "      \"your job is to decide whether it contains factual information that must be verified by external sources\"\n",
    "      \"(Attribution step.), performs logical inference from past steps (Logical step.), or does both (Both.). \"\n",
    "      \"Only output the answer, without explanation.\"),\n",
    "    \"correctness_label\":\n",
    "      (\n",
    "          \"You are a logic expert. Your job is to determine if a reasoning step (the hypothesis) follows logically \"\n",
    "          \"from the previous steps (the premise) and the question. You can assume all statements are correct \"\n",
    "          \"in previous steps. A logically correct step must be a valid inference from the given premises and question. \"\n",
    "          \"If it is a valid logical inference, output 'Correct'. If it does not, output 'Incorrect'. \"\n",
    "          \"Only output the answer, without explanation.\\n\\n\"\n",
    "          \"Here are some examples:\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
    "          \"Premise: 'France and Japan are separated by thousands of kilometers and an ocean.'\\n\"\n",
    "          \"Hypothesis: 'No, you cannot walk from France to Japan.'\\n\"\n",
    "          \"Output: {{Correct, Incorrect}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Correct\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is this statement plausible: John threw a strike out at Little League on Sunday.'\\n\"\n",
    "          \"Premise: 'Little League is a youth baseball organization. Striking out is a common baseball term.'\\n\"\n",
    "          \"Hypothesis: 'No, the statement is not plausible.'\\n\"\n",
    "          \"Output: {{Correct, Incorrect}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Incorrect\"\n",
    "      ),\n",
    "    \"logic_relevance_label\":\n",
    "       (\n",
    "          \"You are a judge evaluating whether each step in a multi-step answer contributes \"\n",
    "          \"meaningfully toward answering the question. Steps that are off-topic or irrelevant \"\n",
    "          \"should be marked 'Not Relevant'. Others should be marked 'Relevant'. Only output the answer, without explanation.\"\n",
    "          \"Here are some examples:\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
    "          \"Answer: 'France and Japan are separated by thousands of kilometers and an ocean. France has more walking areas than Japan. No, you cannot walk from France to Japan.'\\n\"\n",
    "          \"Step: France has more walking areas than Japan.\"\n",
    "          \"Is this step relevant answering the question? Output: {{Relevant, Not Relevant}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Not Relevant\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is this statement plausible: John threw a strike out at Little League on Sunday.'\\n\"\n",
    "          \"Answer: 'Little League is a youth baseball organization. Striking out is a common baseball term. No, the statement is not plausible.'\\n\"\n",
    "          \"Step: Striking out is a common baseball term.\"\n",
    "          \"Is this step relevant to answering the question? Output: {{Relevant, Not Relevant}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Relevant\\n\\n\"\n",
    "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
    "          \"Answer: 'France and Japan are separated by thousands of kilometers and an ocean. France has more walking areas than Japan. So the answer is No.'\\n\"\n",
    "          \"Step: So the answer is No.\"\n",
    "          \"Is this step relevant to answering the quesiton? Output: {{Relevant, Not Relevant}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Relevant\\n\\n\"\n",
    "        ),\n",
    "    \"logic_relevance_label_reflect\": (\n",
    "         \"You are a logic expert reviewing your own reasoning. You must evaluate whether each step in a multi-step answer contributes \"\n",
    "        \"meaningfully toward answering the question. Steps that are off-topic, redundant, or irrelevant \"\n",
    "        \"should be marked 'Not Relevant'. Others should be marked 'Relevant'. You previously said this step is relevant. \"\n",
    "        \"I want you to reflect carefully: Is this step relevant to answering the question? Briefly explain your answer then \"\n",
    "        \"at the end, output your final judgment clearly as either: Relevant or Not Relevant.\"\n",
    "    ),\n",
    "    \"answer_is_logically_correct\": (\n",
    "        \"You are a fact-checking and logic expert. Given a question, a multi-step answer, and supporting evidence, \"\n",
    "        \"determine whether the answer is fully correct both logically and factually. Refer to the provided evidence, if relevant. \"\n",
    "        \"Output 'True' if correct or 'False' if incorrect. Only output the answer, without explanation.\"\n",
    "    ),\n",
    "    \"correctness_label_reflect\": (\n",
    "        \"You are a logic expert reviewing your own reasoning. You claimed that the following reasoning step logically follows \"\n",
    "        \"from earlier steps (the premise) and the question being asked. \"\n",
    "        \"However, you may be wrong, and people are doubting your reasoning. I want you to reflect carefully: \"\n",
    "        \"Does the hypothesis follow logically from the question and the premise? Assume all earlier steps are factually correct.\"\n",
    "        \"Defend your reasoning if it is valid. Otherwise, admit the error. Don't overthink. \"\n",
    "        \"At the end, output your final judgment clearly as either: Correct or Incorrect.\"\n",
    "    ),\n",
    "    \"correctness_label_reflect_confidence\": (\n",
    "        \"You are assessing the validity of your own logical step. Given the question and the preceding, \"\n",
    "        \"factually sound steps (the premise), you previously stated that the following step logically follows. \"\n",
    "        \"Now, critically evaluate the strength of this logical connection. On a scale of 1 to 10 (where 1 is \"\n",
    "        \"'not at all' and 10 is 'absolutely certain'), how confident are you that the hypothesis logically follows? Is it \"\n",
    "        \"a valid inference and is it doing everything correct? Explain your confidence score in 1-3 sentences. Finally, state your overall judgment: Correct or Incorrect.\"\n",
    "    ),\n",
    "    \"correctness_label_reflect_law\": (\n",
    "        \"You are defending yourself in court regarding a statement you made. You claimed that the following reasoning step logically follows \"\n",
    "        \"from earlier steps (the premise) and the question being asked. \"\n",
    "        \"Assuming that all earlier steps are factually correct, is the inference you made in this step correct?\"\n",
    "        \"Provide your defense. Remember, you cannot lie on the stand. Defend your reasoning if it is valid. Otherwise, admit the error. \"\n",
    "        \"Don't overthink. At the end, output your final judgment to the jury clearly as either: Correct or Incorrect. \"\n",
    "    ),\n",
    "    \"alignment\": (\n",
    "        \"You are a judge evaluating whether a set of steps in a multi-step answer is in alignment \"\n",
    "        \"with the answer to the question. Outputs where the answer is not derived from the steps should be marked \"\n",
    "        \"as No, and outputs where the answer does come in alignment with the steps should be marked as Yes. Only output the answer \"\n",
    "        \"without explanation.\"\n",
    "    ),\n",
    "    \"resilience\": (\n",
    "        \"TODO\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e85557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_as_judge(prompt, task, client, prompts, model_family = 'openai', model_type = 'gpt-3.5-turbo'):\n",
    "    res = None\n",
    "    model_family = model_family.lower()\n",
    "    system_prompt = prompts[task]\n",
    "    if 'openai' in model_family:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages= [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        res = response.choices[0].message.content.strip()\n",
    "\n",
    "    elif 'anthropic' in model_family:\n",
    "        response = client.messages.create(\n",
    "            model=model_type,\n",
    "            max_tokens=1000,\n",
    "            system=system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        res = response.content[0].text\n",
    "\n",
    "    elif 'gemini' in model_family:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_type,\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "        res =  response.text\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4fd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_correctness_prompt(question, previous_steps, step):\n",
    "    return f\"\"\"Logic Task\n",
    "            Question: '{question}'\n",
    "            Premise: '{previous_steps}'\n",
    "            Is the following hypothesis a correct logical inference based on the premise and question being asked?\n",
    "            Hypothesis: '{step}'\n",
    "            Output: {{Correct, Incorrect}}\"\"\"\n",
    "\n",
    "def relevance_prompt(question, cot_answer, step):\n",
    "    return f\"\"\"Relevance Task\n",
    "            Question: {question}\n",
    "            Answer: {cot_answer}\n",
    "            Step: {step}\n",
    "            Is this step relevant to answering the question?\n",
    "            Output: {{Relevant, Not Relevant}}\"\"\"\n",
    "\n",
    "def reasoning_answer_alignment_prompt(question, reasoning_steps, answer):\n",
    "    return f\"\"\"Reasoning Task\n",
    "            Question: '{question}'\n",
    "            Reasoning steps: '{reasoning_steps}'\n",
    "            Answer: '{answer}'\n",
    "\n",
    "            Are the reasoning steps in alignment with the answer?\n",
    "            Output: {{Yes, No}}\"\"\"\n",
    "\n",
    "def resilience_prompt(question, question_a, question_b, cot, cot_a, cot_b):\n",
    "    # TODO\n",
    "    return \"TODO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb3330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_response(text, output_type='correctness'):\n",
    "    if output_type == 'correctness':\n",
    "        match = re.search(r\"Output:\\s*(Correct|Incorrect)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).capitalize()  # Returns 'Correct' or 'Incorrect'\n",
    "    elif output_type == 'relevance':\n",
    "        match = re.search(r\"Output:\\s*(Relevant|Not Relevant)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).capitalize()  # Returns 'Relevant' or 'Not Relevant'\n",
    "    return None\n",
    "\n",
    "def evaluate_cot_steps_ensemble(data, clients, model_types):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        grouped_data: dict with (question_id, answer_id, answer_model) -> steps\n",
    "        clients: dict of {model_family: client}\n",
    "        model_types: dict of {model_family: model_name}\n",
    "    Returns:\n",
    "        DataFrame with step-wise and final predictions using majority vote\n",
    "    \"\"\"\n",
    "\n",
    "    def majority_vote(preds):\n",
    "        # strip all punctuation including new lines and make lowercase\n",
    "        preds = [v.lower() for v in preds]\n",
    "        preds = [re.sub(r'[^\\w\\s]', '', v) for v in preds]\n",
    "        tally = collections.Counter(preds)\n",
    "\n",
    "        if len(tally) == 0:\n",
    "            return None\n",
    "        return tally.most_common(1)[0][0]  # Most frequent prediction\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i}/{len(data)}\")\n",
    "        \n",
    "        id = data[i]['id']\n",
    "        question = data[i]['question']\n",
    "        step_info = data[i]['steps']\n",
    "        \n",
    "\n",
    "        if id[-1] == 'a' or id[-1] == 'b':\n",
    "            # augmented data: skip, it will be used later for resilience\n",
    "            continue\n",
    "\n",
    "        # store augmented data: used for resilience metric\n",
    "        question_a = data[i + 1]['question']\n",
    "        question_b = data[i + 2]['question']\n",
    "        step_info_a = data[i + 1]['steps']\n",
    "        step_info_b = data[i + 2]['steps']\n",
    "\n",
    "        for model in step_info: # loop through outputs from different models\n",
    "            steps = step_info[model]\n",
    "            steps_a = step_info_a[model]\n",
    "            steps_b = step_info_b[model]\n",
    "\n",
    "            full_cot = \" \".join(steps)\n",
    "            full_cot_a = \" \".join(steps_a)\n",
    "            full_cot_b = \" \".join(steps_b)\n",
    "            for i, step in enumerate(steps):\n",
    "                prev_steps = ' '.join(steps[:i])\n",
    "                step_text = step\n",
    "\n",
    "                # logic metric\n",
    "                logic_prompt_text = logic_correctness_prompt(question, prev_steps, step_text)\n",
    "                logic_correctness_preds = {}\n",
    "                for model_family, client in clients.items():\n",
    "                    model_type = model_types[model_family]\n",
    "                    logic_output = query_llm_as_judge(logic_prompt_text, \"correctness_label\", client, system_prompts, model_family, model_type)\n",
    "                    # print(logic_prompt_text, logic_output)\n",
    "                    # logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\n",
    "                    logic_output = logic_output.replace(\"\\n\", \"\")\n",
    "                    if logic_output:\n",
    "                        logic_correctness_preds[model_family] = logic_output\n",
    "                \n",
    "                # relevance metric\n",
    "                relevance_preds = {}\n",
    "                relevance_prompt_text = relevance_prompt(question, full_cot, step_text)\n",
    "                for model_family, client in clients.items():\n",
    "                    model_type = model_types[model_family]\n",
    "                    relevance_output = query_llm_as_judge(relevance_prompt_text, \"logic_relevance_label\", client, system_prompts, model_family, model_type)\n",
    "                    # relevance_output = extract_answer_from_response(relevance_output, 'relevance')  # only if using self reflection prompting\n",
    "                    relevance_output = relevance_output.replace(\"\\n\", \"\")\n",
    "                    if relevance_output:\n",
    "                        relevance_preds[model_family] = relevance_output\n",
    "            \n",
    "            # reasoning alignment metric\n",
    "            reasoning_steps = ' '.join(steps[:-1])\n",
    "            answer = steps[-1]\n",
    "            reasoning_alignment_prompt = reasoning_answer_alignment_prompt(question, reasoning_steps, answer)\n",
    "            alignment_preds = {}\n",
    "            for model_family, client in clients.items():\n",
    "                model_type = model_types[model_family]\n",
    "                alignment_output = query_llm_as_judge(reasoning_alignment_prompt, \"alignment\", client, system_prompts, model_family, model_type)\n",
    "\n",
    "                # logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\n",
    "                alignment_output = alignment_output.replace(\"\\n\", \"\")\n",
    "                if alignment_output:\n",
    "                    alignment_preds[model_family] = alignment_output\n",
    "\n",
    "            # resilience metric\n",
    "            resilience_text = resilience_prompt(question, question_a, question_b, full_cot, full_cot_a, full_cot_b)\n",
    "            resilience_preds = {}\n",
    "            for model_family, client in clients.items():\n",
    "                model_type = model_types[model_family]\n",
    "                resilience_output = query_llm_as_judge(resilience_text, \"resilience\", client, system_prompts, model_family, model_type)\n",
    "\n",
    "                # logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\n",
    "                resilience_output = resilience_output.replace(\"\\n\", \"\")\n",
    "                if resilience_output:\n",
    "                    resilience_preds[model_family] = resilience_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93e98b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0/1200\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TODO: Change clients_dict and models_dict based on the LLMs you want to evaluate\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TODO: If no majority vote ensembling, just choose one model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m clients_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai1\u001b[39m\u001b[38;5;124m'\u001b[39m: gpt_client, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai2\u001b[39m\u001b[38;5;124m'\u001b[39m: gpt_client, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m'\u001b[39m: claude_client, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini1\u001b[39m\u001b[38;5;124m'\u001b[39m: gemini_client, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini2\u001b[39m\u001b[38;5;124m'\u001b[39m: gemini_client}\n\u001b[0;32m---> 23\u001b[0m \u001b[43mevaluate_cot_steps_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m, in \u001b[0;36mevaluate_cot_steps_ensemble\u001b[0;34m(data, clients, model_types)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_family, client \u001b[38;5;129;01min\u001b[39;00m clients\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     69\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m model_types[model_family]\n\u001b[0;32m---> 70\u001b[0m     logic_output \u001b[38;5;241m=\u001b[39m \u001b[43mquery_llm_as_judge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogic_prompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorrectness_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_family\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# print(logic_prompt_text, logic_output)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     logic_output \u001b[38;5;241m=\u001b[39m logic_output\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mquery_llm_as_judge\u001b[0;34m(prompt, task, client, prompts, model_family, model_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m prompts[task]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_family:\n\u001b[0;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     res \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_family:\n",
      "File \u001b[0;32m~/Desktop/CS159-LLM-Reasoning/venv/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CS159-LLM-Reasoning/venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CS159-LLM-Reasoning/venv/lib/python3.10/site-packages/openai/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/CS159-LLM-Reasoning/venv/lib/python3.10/site-packages/openai/_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "models_dict = {\n",
    "    'openai1': 'gpt-3.5-turbo', \n",
    "    'openai2': 'gpt-4-turbo', \n",
    "    'anthropic': 'claude-3-haiku-20240307', \n",
    "    'gemini1': 'gemini-1.5-flash',\n",
    "    'gemini2': 'gemini-2.0-flash'\n",
    "}\n",
    "\n",
    "# API Keys\n",
    "MY_OPENAI_KEY = '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'\n",
    "MY_ANTHROPIC_KEY = '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'\n",
    "MY_GEMINI_KEY = 'AIzaSyC13qSGNQ8vMeqwkQdQA1pQ7o4LSBZJBX0'\n",
    "\n",
    "# Connect to APIs\n",
    "gpt_client = OpenAI(api_key = MY_OPENAI_KEY)\n",
    "claude_client = anthropic.Anthropic(api_key = MY_ANTHROPIC_KEY)\n",
    "gemini_client = genai.Client(api_key=MY_GEMINI_KEY)\n",
    "\n",
    "# TODO: Change clients_dict and models_dict based on the LLMs you want to evaluate\n",
    "# TODO: If no majority vote ensembling, just choose one model\n",
    "clients_dict = {'openai1': gpt_client, 'openai2': gpt_client, 'anthropic': claude_client, 'gemini1': gemini_client, 'gemini2': gemini_client}\n",
    "\n",
    "evaluate_cot_steps_ensemble(data, clients_dict, models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c55a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
