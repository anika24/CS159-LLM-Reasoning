{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe903ea6",
   "metadata": {},
   "source": [
    "# Calculate Metrics on BIG-Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d248604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import anthropic\n",
    "import re\n",
    "import collections\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757f2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Keith is 5 feet tall so he is less likely to become an amateur basketball player than a horse jockey.\n",
      "Prompt CoT: Evaluate if the following Q follows common sense. Answer 'True' or 'False'\n",
      "Q: Keith is 5 feet tall so he is less likely to become an amateur basketball player than a horse jockey.\n",
      "In answering this question each step should be on a separate line and start with a number and a period, followed by the reasoning. Finally the answer should be on a new line with the word 'Answer' proceeded by a colon.\n",
      "A: Let's think step by step.\n",
      "1200\n",
      "dict_keys(['id', 'task', 'question', 'answer', 'prompt_direct', 'prompt_cot', 'response', 'steps'])\n"
     ]
    }
   ],
   "source": [
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'data/big_bench_augmented_with_steps.json'\n",
    "data = load_json_file(file_path)\n",
    "\n",
    "if data:\n",
    "    print('Question:', data[0]['question'])\n",
    "    print('Prompt CoT:', data[0]['prompt_cot'])\n",
    "    print(len(data))\n",
    "    print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53371790",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = {\n",
    "     \"correctness_label\":\n",
    "      (\n",
    "          \"You are a logic expert. Your job is to determine if a reasoning step (the hypothesis) follows logically \"\n",
    "          \"from the previous steps (the premise) and the question. You can assume all statements are correct \"\n",
    "          \"in previous steps. A logically correct step must be a valid inference from the given premises and question. \"\n",
    "          \"A mistake can be a step that is logically incorrect or requires big \"\n",
    "          \"logical jumps based on prior steps.\"\n",
    "          \"If it is a valid logical inference, output 'Correct'. If it does not, output 'Incorrect'. \"\n",
    "          \"Only output the answer, without explanation.\\n\\n\"\n",
    "          \"Here are some examples:\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
    "          \"Premise: 'France and Japan are separated by thousands of kilometers and an ocean.'\\n\"\n",
    "          \"Hypothesis: 'No, you cannot walk from France to Japan.'\\n\"\n",
    "          \"Output: {{Correct, Incorrect}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Correct\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is this statement plausible: John threw a strike out at Little League on Sunday.'\\n\"\n",
    "          \"Premise: 'Little League is a youth baseball organization. Striking out is a common baseball term.'\\n\"\n",
    "          \"Hypothesis: 'No, the statement is not plausible.'\\n\"\n",
    "          \"Output: {{Correct, Incorrect}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Incorrect\"\n",
    "      ),\n",
    "    \n",
    "    \"logic_relevance_label\":\n",
    "       (\n",
    "          \"You are a judge evaluating whether each step in a multi-step answer contributes \"\n",
    "          \"meaningfully toward answering the question. Steps that are off-topic, irrelevant, or redundant \"\n",
    "          \"should be marked 'Not Relevant'. Others should be marked 'Relevant'. Only output the answer, without explanation.\"\n",
    "          \"Here are some examples:\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
    "          \"Answer: 'France and Japan are separated by thousands of kilometers and an ocean. France has more walking areas than Japan. No, you cannot walk from France to Japan.'\\n\"\n",
    "          \"Step: France has more walking areas than Japan.\"\n",
    "          \"Is this step relevant answering the question? Output: {{Relevant, Not Relevant}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Not Relevant\\n\\n\"\n",
    "          \"Human:\\n\"\n",
    "          \"Question: 'Is this statement plausible: John threw a strike out at Little League on Sunday.'\\n\"\n",
    "          \"Answer: 'Little League is a youth baseball organization. Striking out is a common baseball term. No, the statement is not plausible.'\\n\"\n",
    "          \"Step: Striking out is a common baseball term.\"\n",
    "          \"Is this step relevant to answering the question? Output: {{Relevant, Not Relevant}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Relevant\\n\\n\"\n",
    "          \"Question: 'Is it possible to walk from France to Japan?'\\n\"\n",
    "          \"Answer: 'France and Japan are separated by thousands of kilometers and an ocean. France has more walking areas than Japan. So the answer is No.'\\n\"\n",
    "          \"Step: So the answer is No.\"\n",
    "          \"Is this step relevant to answering the quesiton? Output: {{Relevant, Not Relevant}}\"\n",
    "          \"Assistant:\\n\"\n",
    "          \"Relevant\\n\\n\"\n",
    "        ),\n",
    " \n",
    "    \"alignment\": (\n",
    "        \"You are a judge evaluating whether a set of steps in a multi-step answer is in alignment \"\n",
    "        \"with the answer to the question. Outputs where the answer is not derived from the steps should be marked \"\n",
    "        \"as No, and outputs where the answer does come in alignment with the steps should be marked as Yes. Only output the answer \"\n",
    "        \"without explanation.\"\n",
    "    ),\n",
    "    \"resilience\": (\n",
    "        \"TODO\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e85557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_as_judge(prompt, task, client, prompts, model_family = 'openai', model_type = 'gpt-3.5-turbo'):\n",
    "    res = None\n",
    "    model_family = model_family.lower()\n",
    "    system_prompt = prompts[task]\n",
    "    if 'openai' in model_family:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_type,\n",
    "            messages= [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        res = response.choices[0].message.content.strip()\n",
    "\n",
    "    elif 'anthropic' in model_family:\n",
    "        response = client.messages.create(\n",
    "            model=model_type,\n",
    "            max_tokens=1000,\n",
    "            system=system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        res = response.content[0].text\n",
    "\n",
    "    elif 'gemini' in model_family:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_type,\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "        res =  response.text\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4fd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_correctness_prompt(question, previous_steps, step):\n",
    "    return f\"\"\"Logic Task\n",
    "            Question: '{question}'\n",
    "            Premise: '{previous_steps}'\n",
    "            Is the following hypothesis a correct logical inference based on the premise and question being asked?\n",
    "            Hypothesis: '{step}'\n",
    "            Output: {{Correct, Incorrect}}\"\"\"\n",
    "\n",
    "def relevance_prompt(question, cot_answer, step):\n",
    "    return f\"\"\"Relevance Task\n",
    "            Question: {question}\n",
    "            Answer: {cot_answer}\n",
    "            Step: {step}\n",
    "            Is this step relevant to answering the question?\n",
    "            Output: {{Relevant, Not Relevant}}\"\"\"\n",
    "\n",
    "def reasoning_answer_alignment_prompt(question, reasoning_steps, answer):\n",
    "    return f\"\"\"Reasoning Task\n",
    "            Question: '{question}'\n",
    "            Reasoning steps: '{reasoning_steps}'\n",
    "            Answer: '{answer}'\n",
    "\n",
    "            Are the reasoning steps in alignment with the answer?\n",
    "            Output: {{Yes, No}}\"\"\"\n",
    "\n",
    "def resilience_prompt(question, question_a, question_b, cot, cot_a, cot_b):\n",
    "    # TODO\n",
    "    return \"TODO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbb3330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_response(text, output_type='correctness'):\n",
    "    if output_type == 'correctness':\n",
    "        match = re.search(r\"Output:\\s*(Correct|Incorrect)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).capitalize()  # Returns 'Correct' or 'Incorrect'\n",
    "    elif output_type == 'relevance':\n",
    "        match = re.search(r\"Output:\\s*(Relevant|Not Relevant)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).capitalize()  # Returns 'Relevant' or 'Not Relevant'\n",
    "    return None\n",
    "\n",
    "def evaluate_cot_steps_ensemble(data, clients, model_types, models_to_evaluate):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        grouped_data: dict with (question_id, answer_id, answer_model) -> steps\n",
    "        clients: dict of {model_family: client}\n",
    "        model_types: dict of {model_family: model_name}\n",
    "        models_to_evaluate: list of model names to evaluate the responses of\n",
    "    Returns:\n",
    "        DataFrame with step-wise and final predictions using majority vote\n",
    "    \"\"\"\n",
    "\n",
    "    def majority_vote(preds):\n",
    "        # strip all punctuation including new lines and make lowercase\n",
    "        preds = [v.lower() for v in preds]\n",
    "        preds = [re.sub(r'[^\\w\\s]', '', v) for v in preds]\n",
    "        tally = collections.Counter(preds)\n",
    "\n",
    "        if len(tally) == 0:\n",
    "            return None\n",
    "        return tally.most_common(1)[0][0]  # Most frequent prediction\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i}/{len(data)}\")\n",
    "        \n",
    "        id = data[i]['id']\n",
    "        question = data[i]['question']\n",
    "        step_info = data[i]['steps']\n",
    "        true_answer = data[i]['answer']\n",
    "        \n",
    "\n",
    "        if id[-1] == 'a' or id[-1] == 'b':\n",
    "            # augmented data: skip, it will be used later for resilience\n",
    "            continue\n",
    "\n",
    "        # store augmented data: used for resilience metric\n",
    "        question_a = data[i + 1]['question']\n",
    "        question_b = data[i + 2]['question']\n",
    "        step_info_a = data[i + 1]['steps']\n",
    "        step_info_b = data[i + 2]['steps']\n",
    "\n",
    "        for model in models_to_evaluate: # loop through outputs from different models\n",
    "            steps = step_info[model]\n",
    "            pred_answer = steps[-1]\n",
    "            steps_a = step_info_a[model]\n",
    "            steps_b = step_info_b[model]\n",
    "            \n",
    "            # Remove any None values from steps\n",
    "            steps = [step for step in steps if step is not None]\n",
    "            steps_a = [step for step in steps_a if step is not None]\n",
    "            steps_b = [step for step in steps_b if step is not None]\n",
    "\n",
    "            full_cot = \" \".join(steps)\n",
    "            full_cot_a = \" \".join(steps_a)\n",
    "            full_cot_b = \" \".join(steps_b)\n",
    "            correctness_annotations = []\n",
    "            relevance_annotations = []\n",
    "            \n",
    "            for i, step in enumerate(steps):\n",
    "                prev_steps = ' '.join(steps[:i])\n",
    "                step_text = step\n",
    "\n",
    "                # logic metric\n",
    "                logic_prompt_text = logic_correctness_prompt(question, prev_steps, step_text)\n",
    "                print(logic_prompt_text)\n",
    "                logic_correctness_preds = {}\n",
    "                for model_family, client in clients.items():\n",
    "                    model_type = model_types[model_family]\n",
    "                    logic_output = query_llm_as_judge(logic_prompt_text, \"correctness_label\", client, system_prompts, model_family, model_type)\n",
    "                    logic_output = logic_output.replace(\"\\n\", \"\")\n",
    "                    if logic_output:\n",
    "                        logic_correctness_preds[model_family] = logic_output\n",
    "                \n",
    "                # relevance metric\n",
    "                relevance_preds = {}\n",
    "                relevance_prompt_text = relevance_prompt(question, full_cot, step_text)\n",
    "                for model_family, client in clients.items():\n",
    "                    model_type = model_types[model_family]\n",
    "                    relevance_output = query_llm_as_judge(relevance_prompt_text, \"logic_relevance_label\", client, system_prompts, model_family, model_type)\n",
    "                    relevance_output = relevance_output.replace(\"\\n\", \"\")\n",
    "                    if relevance_output:\n",
    "                        relevance_preds[model_family] = relevance_output\n",
    "\n",
    "                correctness_annotations.append(majority_vote(list(logic_correctness_preds.values())))\n",
    "                relevance_annotations.append(majority_vote(list(relevance_preds.values())))\n",
    "            \n",
    "            # reasoning alignment metric\n",
    "            reasoning_steps = ' '.join(steps[:-1])\n",
    "            answer = steps[-1]\n",
    "            reasoning_alignment_prompt = reasoning_answer_alignment_prompt(question, reasoning_steps, answer)\n",
    "            alignment_preds = {}\n",
    "            for model_family, client in clients.items():\n",
    "                model_type = model_types[model_family]\n",
    "                alignment_output = query_llm_as_judge(reasoning_alignment_prompt, \"alignment\", client, system_prompts, model_family, model_type)\n",
    "                alignment_output = alignment_output.replace(\"\\n\", \"\")\n",
    "                if alignment_output:\n",
    "                    alignment_preds[model_family] = alignment_output\n",
    "\n",
    "            # # resilience metric\n",
    "            # resilience_text = resilience_prompt(question, question_a, question_b, full_cot, full_cot_a, full_cot_b)\n",
    "            # resilience_preds = {}\n",
    "            # for model_family, client in clients.items():\n",
    "            #     model_type = model_types[model_family]\n",
    "            #     resilience_output = query_llm_as_judge(resilience_text, \"resilience\", client, system_prompts, model_family, model_type)\n",
    "\n",
    "            #     # logic_output = extract_answer_from_response(logic_output, 'correctness') # only if using self reflection prompting\n",
    "            #     resilience_output = resilience_output.replace(\"\\n\", \"\")\n",
    "            #     if resilience_output:\n",
    "            #         resilience_preds[model_family] = resilience_output\n",
    "                    \n",
    "            results.append({\n",
    "                'id': id,\n",
    "                'model': model,\n",
    "                'question': question,\n",
    "                'cot_steps': steps,\n",
    "                'correctness_annotations': correctness_annotations,\n",
    "                'relevance_annotations': relevance_annotations,\n",
    "                'alignment_preds': majority_vote(list(alignment_preds.values())),\n",
    "                # 'resilience_preds': resilience_preds,\n",
    "                'true_answer': true_answer,\n",
    "                'pred_answer': pred_answer,\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e98b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_evaluate = ['gpt-3.5-turbo', 'gpt-4-turbo', 'claude-3-haiku-20240307', 'gemini-1.5-flash', 'gemini-2.0-flash']\n",
    "models_to_evaluate = ['gpt-3.5-turbo', 'gpt-4-turbo', 'claude-3-haiku-20240307'] # we can choose a subset of model responses to evaluate\n",
    "\n",
    "# API Keys\n",
    "MY_OPENAI_KEY = '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'\n",
    "MY_ANTHROPIC_KEY = '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'\n",
    "MY_GEMINI_KEY = 'AIzaSyC13qSGNQ8vMeqwkQdQA1pQ7o4LSBZJBX0'\n",
    "\n",
    "# Connect to APIs\n",
    "gpt_client = OpenAI(api_key = MY_OPENAI_KEY)\n",
    "claude_client = anthropic.Anthropic(api_key = MY_ANTHROPIC_KEY)\n",
    "gemini_client = genai.Client(api_key=MY_GEMINI_KEY)\n",
    "\n",
    "# No majority vote ensembling\n",
    "clients_dict = {'gemini': gemini_client}\n",
    "models_dict = {'gemini': 'gemini-1.5-flash'}\n",
    "\n",
    "com2sense_data = [data[i] for i in range(len(data)) if data[i]['task'] == 'com2sense']\n",
    "n = 6 # MULTIPLE OF 3\n",
    "data_subset = com2sense_data[:n]\n",
    "\n",
    "results = evaluate_cot_steps_ensemble(data_subset, clients_dict, models_dict, models_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9424646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>question</th>\n",
       "      <th>cot_steps</th>\n",
       "      <th>correctness_annotations</th>\n",
       "      <th>relevance_annotations</th>\n",
       "      <th>alignment_preds</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>pred_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com2sense_0</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>Keith is 5 feet tall so he is less likely to b...</td>\n",
       "      <td>[Being 5 feet tall is on the shorter side for ...</td>\n",
       "      <td>[correct, correct, correct, correct, correct]</td>\n",
       "      <td>[relevant, relevant, relevant, relevant, relev...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com2sense_0</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>Keith is 5 feet tall so he is less likely to b...</td>\n",
       "      <td>[Consider the typical height requirements for ...</td>\n",
       "      <td>[correct, correct, correct, correct, correct, ...</td>\n",
       "      <td>[relevant, relevant, relevant, relevant, relev...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com2sense_0</td>\n",
       "      <td>claude-3-haiku-20240307</td>\n",
       "      <td>Keith is 5 feet tall so he is less likely to b...</td>\n",
       "      <td>[Keith is 5 feet tall, which is considered rel...</td>\n",
       "      <td>[correct, correct, correct, correct, correct]</td>\n",
       "      <td>[relevant, relevant, relevant, relevant, relev...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com2sense_1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>Sally needs to be at work in 5 minutes while M...</td>\n",
       "      <td>[Sally needs to be at work in 5 minutes while ...</td>\n",
       "      <td>[correct, correct, correct, correct, correct, ...</td>\n",
       "      <td>[relevant, relevant, relevant, relevant, relev...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com2sense_1</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>Sally needs to be at work in 5 minutes while M...</td>\n",
       "      <td>[Sally only has 5 minutes to get to work, whil...</td>\n",
       "      <td>[correct, correct, correct, correct, correct, ...</td>\n",
       "      <td>[relevant, relevant, relevant, relevant, relev...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>com2sense_1</td>\n",
       "      <td>claude-3-haiku-20240307</td>\n",
       "      <td>Sally needs to be at work in 5 minutes while M...</td>\n",
       "      <td>[Sally needs to be at work in 5 minutes, while...</td>\n",
       "      <td>[correct, correct, correct, correct, incorrect...</td>\n",
       "      <td>[relevant, relevant, relevant, relevant, relev...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                    model  \\\n",
       "0  com2sense_0            gpt-3.5-turbo   \n",
       "1  com2sense_0              gpt-4-turbo   \n",
       "2  com2sense_0  claude-3-haiku-20240307   \n",
       "3  com2sense_1            gpt-3.5-turbo   \n",
       "4  com2sense_1              gpt-4-turbo   \n",
       "5  com2sense_1  claude-3-haiku-20240307   \n",
       "\n",
       "                                            question  \\\n",
       "0  Keith is 5 feet tall so he is less likely to b...   \n",
       "1  Keith is 5 feet tall so he is less likely to b...   \n",
       "2  Keith is 5 feet tall so he is less likely to b...   \n",
       "3  Sally needs to be at work in 5 minutes while M...   \n",
       "4  Sally needs to be at work in 5 minutes while M...   \n",
       "5  Sally needs to be at work in 5 minutes while M...   \n",
       "\n",
       "                                           cot_steps  \\\n",
       "0  [Being 5 feet tall is on the shorter side for ...   \n",
       "1  [Consider the typical height requirements for ...   \n",
       "2  [Keith is 5 feet tall, which is considered rel...   \n",
       "3  [Sally needs to be at work in 5 minutes while ...   \n",
       "4  [Sally only has 5 minutes to get to work, whil...   \n",
       "5  [Sally needs to be at work in 5 minutes, while...   \n",
       "\n",
       "                             correctness_annotations  \\\n",
       "0      [correct, correct, correct, correct, correct]   \n",
       "1  [correct, correct, correct, correct, correct, ...   \n",
       "2      [correct, correct, correct, correct, correct]   \n",
       "3  [correct, correct, correct, correct, correct, ...   \n",
       "4  [correct, correct, correct, correct, correct, ...   \n",
       "5  [correct, correct, correct, correct, incorrect...   \n",
       "\n",
       "                               relevance_annotations alignment_preds  \\\n",
       "0  [relevant, relevant, relevant, relevant, relev...             yes   \n",
       "1  [relevant, relevant, relevant, relevant, relev...             yes   \n",
       "2  [relevant, relevant, relevant, relevant, relev...             yes   \n",
       "3  [relevant, relevant, relevant, relevant, relev...             yes   \n",
       "4  [relevant, relevant, relevant, relevant, relev...             yes   \n",
       "5  [relevant, relevant, relevant, relevant, relev...             yes   \n",
       "\n",
       "   true_answer pred_answer  \n",
       "0         True        True  \n",
       "1         True        True  \n",
       "2         True        True  \n",
       "3         True        True  \n",
       "4         True        True  \n",
       "5         True       False  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert results to dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32447244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Sally needs to be at work in 5 minutes while Mark has 10 to spare, so it makes more sense for Sally to rush and forget doing the dish.\n",
      "['Sally needs to be at work in 5 minutes, while Mark has 10 to spare.', 'This suggests that Sally is in a more time-constrained situation than Mark.', 'Rushing to work and forgetting to do the dish may help Sally get to work on time, but it could lead to other consequences, such as not completing a necessary task or potentially causing stress or guilt.', 'Mark, on the other hand, has more time available, so it would make more sense for him to complete the dish while Sally rushes to work.', 'Therefore, it does not make more sense for Sally to rush and forget doing the dish, as that would not be the most logical or considerate course of action.', 'False']\n",
      "Incorrect step:  5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results_df)):\n",
    "    assert len(results_df.loc[i, 'correctness_annotations']) == len(results_df.loc[i, 'relevance_annotations'])\n",
    "    # count how many 'incorrect' are in correctness annotations\n",
    "    incorrect_count = 0\n",
    "    irrelevant_count = 0\n",
    "    for j, correctness in enumerate(results_df.loc[i, 'correctness_annotations']):\n",
    "        if correctness == 'incorrect':\n",
    "            print('Question: ', results_df['question'][i])\n",
    "            print([step for step in results_df['cot_steps'][i]])\n",
    "            print('Incorrect step: ', j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d50179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
